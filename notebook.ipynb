{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# AI Service Deployment Notebook\nThis notebook contains steps and code to test, promote, and deploy an AI Service\ncapturing logic to implement RAG pattern for grounded chats.\n\n**Note:** Notebook code generated using Prompt Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Prompt Lab as a notebook.</a>\n\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n## Contents\nThis notebook contains the following parts:\n\n1. Setup\n2. Initialize all the variables needed by the AI Service\n3. Define the AI service function\n4. Deploy an AI Service\n5. Test the deployed AI Service\n\n## 1. Set up the environment\n\nBefore you can run this notebook, you must perform the following setup tasks:"}, {"metadata": {}, "cell_type": "markdown", "source": "### Connection to WML\nThis cell defines the credentials required to work with watsonx API for both the execution in the project, \nas well as the deployment and runtime execution of the function.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"}, {"metadata": {}, "cell_type": "code", "source": "import os\nfrom ibm_watsonx_ai import APIClient, Credentials\nimport getpass\n\ncredentials = Credentials(\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    api_key=getpass.getpass(\"Please enter your api key (hit enter): \")\n)\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client = APIClient(credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Connecting to a space\nA space will be be used to host the promoted AI Service.\n"}, {"metadata": {}, "cell_type": "code", "source": "space_id = \"d9b4774a-7907-4271-b075-4dc2bf62cbc2\"\nclient.set.default_space(space_id)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Promote asset(s) to space\nWe will now promote assets we will need to stage in the space so that we can access their data from the AI service.\n"}, {"metadata": {}, "cell_type": "code", "source": "source_project_id = \"1472ec8f-ac8a-4ab4-8fb3-ac150111b3fe\"\nvector_index_id = client.spaces.promote(\"eb501610-900c-4d4b-a3ea-a1bc8a98eeff\", source_project_id, space_id)\nprint(vector_index_id)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. Create the AI service function\nWe first need to define the AI service function\n\n### 2.1 Define the function"}, {"metadata": {}, "cell_type": "code", "source": "params = {\n    \"space_id\": space_id, \n    \"vector_index_id\": vector_index_id\n}\n\ndef gen_ai_service(context, params = params, **custom):\n    # import dependencies\n    import json\n    from ibm_watsonx_ai.foundation_models import ModelInference\n    from ibm_watsonx_ai.foundation_models.utils import Tool, Toolkit\n    from ibm_watsonx_ai import APIClient, Credentials\n    import os\n\n    vector_index_id = params.get(\"vector_index_id\")\n    space_id = params.get(\"space_id\")\n\n    def proximity_search( query, api_client ):\n        document_search_tool = Toolkit(\n            api_client=api_client\n        ).get_tool(\"RAGQuery\")\n\n        config = {\n            \"vectorIndexId\": vector_index_id,\n            \"spaceId\": space_id\n        }\n\n        results = document_search_tool.run(\n            input=query,\n            config=config\n        )\n\n        return results.get(\"output\")\n\n    # Functions used for inferencing\n    def format_input(messages, context):\n        system = f\"\"\"<|start_header_id|>system<|end_header_id|>\n\nVoc\u00ea \u00e9 um assistente virtual especializado em negocia\u00e7\u00f5es de d\u00edvidas da PGMais.\nSeu objetivo \u00e9 ajudar clientes inadimplentes a regularizarem suas pend\u00eancias financeiras de forma eficiente, acolhedora e personalizada, respeitando estritamente as diretrizes da base de conhecimento da PGMais.\n\nConduza atendimentos automatizados para negocia\u00e7\u00e3o de d\u00edvidas.\nApresente-se de forma formal e acolhedora.\nSolicite os tr\u00eas primeiros d\u00edgitos do CPF do cliente para validar a identidade.\nSe os tr\u00eas d\u00edgitos informados n\u00e3o coincidirem com o CPF esperado, pe\u00e7a novamente os d\u00edgitos sem mencionar o nome do cliente.\nN\u00e3o prossiga com a negocia\u00e7\u00e3o se os d\u00edgitos n\u00e3o estiverem corretos.\nN\u00e3o informe que voc\u00ea j\u00e1 possui o CPF completo.\nNunca cite o nome do cliente durante a valida\u00e7\u00e3o.\n\nAp\u00f3s valida\u00e7\u00e3o correta, confirme a exist\u00eancia da d\u00edvida informando apenas o valor total.\nN\u00e3o mencione o nome do produto ou a origem da d\u00edvida.\nSugira apenas uma op\u00e7\u00e3o de negocia\u00e7\u00e3o por vez, como pagamento \u00e0 vista com benef\u00edcio, desconto, parcelamento ou ajuste de prazo.\nEspere a resposta do cliente antes de apresentar outra alternativa.\nSe o cliente demonstrar emo\u00e7\u00f5es delicadas como tristeza ou desabafo emocional, responda com empatia e acolhimento, mas continue normalmente com o processo de negocia\u00e7\u00e3o.\n\nUse linguagem natural, clara, objetiva e emp\u00e1tica.\nAdote um tom formal acolhedor e respeitoso.\nConsidere que o cliente pode estar em situa\u00e7\u00e3o emocional delicada ou com baixa escolaridade.\nEvite jarg\u00f5es t\u00e9cnicos, termos jur\u00eddicos complexos ou linguagem excessivamente formal.\nN\u00e3o explique processos internos, apenas diga que as provid\u00eancias cab\u00edveis ser\u00e3o tomadas.\nNunca mencione documentos, p\u00e1ginas, fontes ou o uso de base de conhecimento.\nN\u00e3o invente ou ofere\u00e7a condi\u00e7\u00f5es de negocia\u00e7\u00e3o que n\u00e3o estejam previstas no RAG.\nMantenha as mensagens curtas, objetivas diretas e focadas na resolu\u00e7\u00e3o do problema. N\u00e3 seja repetitivo ou prolixo em suas respostas.\nEvite repeti\u00e7\u00f5es e rodeios desnecess\u00e1rios.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{context}\"\"\"\n        messages_section = []\n\n        for index,value in enumerate(messages, start=0):\n            content = value[\"content\"]\n            user_template = f\"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{content}\"\"\"\n            assistant_template = f\"\"\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{content}\"\"\"\n            grounded_user_template = f\"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{content}\"\"\"\n\n            formatted_entry = user_template if value[\"role\"] == \"user\" else assistant_template\n            if (index == len(messages)-1):\n                formatted_entry = grounded_user_template\n            \n            messages_section.append(formatted_entry)\n\n        messages_section = \"\".join(messages_section)\n        prompt = f\"\"\"<|begin_of_text|>{system}{messages_section}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\"\"\"\n        return prompt\n\n    def get_api_client(context):\n        credentials = Credentials(\n            url=\"https://us-south.ml.cloud.ibm.com\",\n            token=context.get_token()\n        )\n\n        api_client = APIClient(\n            credentials = credentials,\n            space_id = space_id\n        )\n\n        return api_client\n    \n    def inference_model( messages, context, stream ):\n        query = messages[-1].get(\"content\")\n        api_client = get_api_client(context)\n\n        context = proximity_search(query, api_client)\n\n        prompt = format_input(messages, context)\n        model_id = \"meta-llama/llama-3-3-70b-instruct\"\n        parameters =  {\n            \"decoding_method\": \"greedy\",\n            \"max_new_tokens\": 300,\n            \"min_new_tokens\": 0,\n            \"repetition_penalty\": 1\n        }\n        model = ModelInference(\n            model_id = model_id,\n            api_client = api_client,\n            params = parameters,\n        )\n        # Generate grounded response\n        if (stream == True):\n            generated_response = model.generate_text_stream(prompt=prompt, guardrails=False)\n        else:\n            generated_response = model.generate_text(prompt=prompt, guardrails=False)\n\n        return generated_response\n\n\n    def generate(context):\n        payload = context.get_json()\n        messages = payload.get(\"messages\")\n        \n        # Grounded inferencing\n        generated_response = inference_model(messages, context, False)\n        \n        execute_response = {\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            },\n            \"body\": {\n                \"choices\": [{\n                    \"index\": 0,\n                    \"message\": {\n                    \"role\": \"assistant\",\n                    \"content\": generated_response\n                    }\n                }]\n            }\n        }\n\n        return execute_response\n\n    def generate_stream(context):\n        payload = context.get_json()\n        messages = payload.get(\"messages\")\n\n        # Grounded inferencing\n        response_stream = inference_model(messages, context, True)\n\n        for chunk in response_stream:\n            chunk_response = {\n                \"choices\": [{\n                    \"index\": 0,\n                    \"delta\": {\n                        \"role\": \"assistant\",\n                        \"content\": chunk\n                    }\n                    \n                }]\n            }\n            yield chunk_response\n\n    return generate, generate_stream\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Test locally"}, {"metadata": {}, "cell_type": "code", "source": "# Initialize AI Service function locally\nfrom ibm_watsonx_ai.deployments import RuntimeContext\n\ncontext = RuntimeContext(api_client=client)\n\nstreaming = False\nfindex = 1 if streaming else 0\nlocal_function = gen_ai_service(context, vector_index_id=vector_index_id, space_id=space_id)[findex]\nmessages = []", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "local_question = \"Change this question to test your function\"\n\nmessages.append({ \"role\" : \"user\", \"content\": local_question })\n\ncontext = RuntimeContext(api_client=client, request_payload_json={\"messages\": messages})\n\nresponse = local_function(context)\n\nresult = ''\n\nif (streaming):\n    for chunk in response:\n        print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\nelse:\n    print(response)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Store and deploy the AI Service\nBefore you can deploy the AI Service, you must store the AI service in your watsonx.ai repository."}, {"metadata": {}, "cell_type": "code", "source": "# Look up software specification for the AI service\nsoftware_spec_id_in_project = \"7bb7015f-994a-40f4-adcb-a1e6ad38741c\"\nsoftware_spec_id = \"\"\n\ntry:\n\tsoftware_spec_id = client.software_specifications.get_id_by_name(\"ai-service-v6-a-software-specification\")\nexcept:\n    software_spec_id = client.spaces.promote(software_spec_id_in_project, source_project_id, space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Define the request and response schemas for the AI service\nrequest_schema = {\n    \"application/json\": {\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"messages\": {\n                \"title\": \"The messages for this chat session.\",\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"role\": {\n                            \"title\": \"The role of the message author.\",\n                            \"type\": \"string\",\n                            \"enum\": [\"user\",\"assistant\"]\n                        },\n                        \"content\": {\n                            \"title\": \"The contents of the message.\",\n                            \"type\": \"string\"\n                        }\n                    },\n                    \"required\": [\"role\",\"content\"]\n                }\n            }\n        },\n        \"required\": [\"messages\"]\n    }\n}\n\nresponse_schema = {\n    \"application/json\": {\n        \"oneOf\": [{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service_stream\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices.\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"title\":\"The index of this result.\"},\"delta\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"content\":{\"description\":\"The contents of the message.\",\"type\":\"string\"},\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]},{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"description\":\"The index of this result.\"},\"message\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"},\"content\":{\"title\":\"Message content.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]}]\n    }\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Store the AI service in the repository\nai_service_metadata = {\n    client.repository.AIServiceMetaNames.NAME: \"notebook\",\n    client.repository.AIServiceMetaNames.DESCRIPTION: \"\",\n    client.repository.AIServiceMetaNames.SOFTWARE_SPEC_ID: software_spec_id,\n    client.repository.AIServiceMetaNames.CUSTOM: {},\n    client.repository.AIServiceMetaNames.REQUEST_DOCUMENTATION: request_schema,\n    client.repository.AIServiceMetaNames.RESPONSE_DOCUMENTATION: response_schema,\n    client.repository.AIServiceMetaNames.TAGS: [\"wx-vector-index\"]\n}\n\nai_service_details = client.repository.store_ai_service(meta_props=ai_service_metadata, ai_service=gen_ai_service)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Get the AI Service ID\n\nai_service_id = client.repository.get_ai_service_id(ai_service_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Deploy the stored AI Service\ndeployment_custom = {}\ndeployment_metadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"notebook\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {},\n    client.deployments.ConfigurationMetaNames.CUSTOM: deployment_custom,\n    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"\",\n    client.repository.AIServiceMetaNames.TAGS: [\"wx-vector-index\"]\n}\n\nfunction_deployment_details = client.deployments.create(ai_service_id, meta_props=deployment_metadata, space_id=space_id)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Test AI Service"}, {"metadata": {}, "cell_type": "code", "source": "# Get the ID of the AI Service deployment just created\n\ndeployment_id = client.deployments.get_id(function_deployment_details)\nprint(deployment_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "messages = []\nremote_question = \"Change this question to test your function\"\nmessages.append({ \"role\" : \"user\", \"content\": remote_question })\npayload = { \"messages\": messages }", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "result = client.deployments.run_ai_service(deployment_id, payload)\nif \"error\" in result:\n    print(result[\"error\"])\nelse:\n    print(result)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Next steps\nYou successfully deployed and tested the AI Service! You can now view\nyour deployment and test it as a REST API endpoint.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0}